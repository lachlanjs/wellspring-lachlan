---
description: 
draft: false
tags:
  - journal
date: ""
---
*federated learning headaches*

---

My 13.6" laptop display feels a lot bigger today for some reason.

I've got a ML problem that I am working on and I thought it would be a fun experiment to document the progress of solving it throughout the day here. I won't explain all of the context - leave that for another page. I'll explain some

The problem has to do with Federated Learning. In essence, as far as I am concerned right now, Federated Learning is about training a series of $N$ models on $N$ different datasets as well as possible. Federated Learning is much more than that in reality. 

The premise is usually that those $N$ datasets have something in common that would make you want to combine all of them together by instinct and train one monolithic model. The model would be better for having been trained on data of higher quality, quantity and diversity. In practice, there is some reason that this cannot occur. You are forced to find a way to train $N$ models in a way that benefits from the $N$ different datasets without ever actually sharing the data explicitly.

Federated Learning solves this problem by executing training epochs for the $N$ different models only on their respective datasets, interrupting this process at regular intervals to **aggregate** and **redistribute** the **models**. In unrigorous terms, this ensures that each model is *influenced by* the $N-1$ datasets that it does not belong to, without any data-sharing explicitly taking place.

What makes the problem interesting, is that these datasets 