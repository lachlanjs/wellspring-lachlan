---
description: 
draft: false
tags: 
date: ""
---
*thinking about AI, as usual*

---
It's a shame to watch knee-jerk jealous vitriol stir in the global AI community in response of the recent Nobel Prize announcements, two of which have recognised AI researchers in non-computing fields. The really surprising thing is how it has appeared to come from both "sides". No one sensibly sets out to get themselves a Nobel prize - it's too fickle - the only sensible thing to do is congratulate their original contributions.

---
[[The Mark of AGI]]

---
I've come up with a new sub-project for my research project, which I am currently acronyming RAFA (no connection to the tennis player who recently announced his retirement).

It stands for Reinforcement Accelerated Feedback Alignment, and I am hoping that it proves to be a novel training algorithm for neural networks. If it works I'll spray all of my thinking and progress on this website. It shouldn't be long before I get to test the idea. The holy grail that I am chasing is a biologically plausible learning algorithm that doesn't involve gradients that can ideally train non-differentiable models. 

If it works, I'm going to apply it to the Embryogenic Networks that I have been working on so far for the project. My theory is that they haven't fully worked on certain tasks because the weights of the models need more fine tuning. The trouble is that backprop will never work on them because the models are not tractably differentiable as a means for solving credit assignment. So a training algorithm that does credit assignment without any need for gradients would be absolutely perfect eh. Guess we will see.